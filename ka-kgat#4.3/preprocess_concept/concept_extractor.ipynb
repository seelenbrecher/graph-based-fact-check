{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding patterns to Matcher.: 100%|██████████| 781267/781267 [00:18<00:00, 42620.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from concept_extractor import get_nlp_and_matcher\n",
    "nlp, matcher = get_nlp_and_matcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import sys\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"paths.cfg\")\n",
    "with open(config[\"paths\"][\"concept_vocab\"], \"r\", encoding=\"utf8\") as f:\n",
    "    cpnet_vocab = [l.strip() for l in list(f.readlines())]\n",
    "cpnet_vocab = [c.replace(\"_\", \" \") for c in cpnet_vocab]\n",
    "\n",
    "\n",
    "blacklist = set([\"-PRON-\", \"actually\", \"likely\", \"possibly\", \"want\",\n",
    "                 \"make\", \"my\", \"someone\", \"sometimes_people\", \"sometimes\",\"would\", \"want_to\",\n",
    "                 \"one\", \"something\", \"sometimes\", \"everybody\", \"somebody\", \"could\", \"could_be\"\n",
    "                 ])\n",
    "\n",
    "def lemmatize(nlp, concept):\n",
    "\n",
    "    doc = nlp(concept.replace(\"_\",\" \"))\n",
    "    lcs = set()\n",
    "    lcs.add(\"_\".join([token.lemma_ for token in doc])) # all lemma\n",
    "    return lcs\n",
    "\n",
    "def load_concept_vocab():\n",
    "    vocab = []\n",
    "    with open(config[\"paths\"][\"concept_vocab\"], \"r\", encoding=\"utf8\") as f:\n",
    "        vocab = [l.strip() for l in list(f.readlines())]\n",
    "    concept2id = {}\n",
    "    id2concept = {}\n",
    "    for indice, cp in enumerate(vocab):\n",
    "        concept2id[cp] = indice\n",
    "        id2concept[indice] = cp\n",
    "    return concept2id, id2concept\n",
    "\n",
    "concept2id, id2concept = load_concept_vocab()\n",
    "\n",
    "\n",
    "def lemmatize(nlp, concept):\n",
    "\n",
    "    doc = nlp(concept.replace(\"_\",\" \"))\n",
    "    lcs = set()\n",
    "    lcs.add(\"_\".join([token.lemma_ for token in doc])) # all lemma\n",
    "    return lcs\n",
    "\n",
    "def load_matcher(nlp):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"paths.cfg\")\n",
    "    with open(config[\"paths\"][\"matcher_patterns\"], \"r\", encoding=\"utf8\") as f:\n",
    "        all_patterns = json.load(f)\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for concept, pattern in tqdm(all_patterns.items(), desc=\"Adding patterns to Matcher.\"):\n",
    "        matcher.add(concept, None, pattern)\n",
    "    return matcher\n",
    "\n",
    "def ground_mentioned_concepts(nlp, matcher, s, ans = \"\"):\n",
    "    global concept2id\n",
    "    s = s.lower()\n",
    "    doc = nlp(s)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    mentioned_concepts = {}\n",
    "    span_to_concepts = {}\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end].text  # the matched span\n",
    "        if len(set(span.split(\" \")).intersection(set(ans.split(\" \")))) > 0:\n",
    "            continue\n",
    "        original_concept = nlp.vocab.strings[match_id]\n",
    "        # print(\"Matched '\" + span + \"' to the rule '\" + string_id)\n",
    "\n",
    "        if len(original_concept.split(\"_\")) == 1:\n",
    "            original_concept = list(lemmatize(nlp, original_concept))[0]\n",
    "\n",
    "        if span not in span_to_concepts:\n",
    "            span_to_concepts[span] = set()\n",
    "\n",
    "        span_to_concepts[span].add(original_concept)\n",
    "\n",
    "    for span, concepts in span_to_concepts.items():\n",
    "        concepts_sorted = list(concepts)\n",
    "        concepts_sorted.sort(key=len)\n",
    "\n",
    "        # mentioned_concepts.update(concepts_sorted[0:2])\n",
    "\n",
    "        shortest = concepts_sorted[0:3] #\n",
    "        for c in shortest:\n",
    "            if c in blacklist:\n",
    "                continue\n",
    "            lcs = lemmatize(nlp, c)\n",
    "            intersect = lcs.intersection(shortest)\n",
    "            if len(intersect)>0:\n",
    "                c = list(intersect)[0]\n",
    "                if c in concept2id:\n",
    "                    mentioned_concepts[span] = c\n",
    "                    break\n",
    "            else:\n",
    "                if c in concept2id:\n",
    "                    mentioned_concepts[span] = c\n",
    "                    break\n",
    "\n",
    "    \n",
    "    mentioned_concepts_with_indices = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end].text\n",
    "        if span in mentioned_concepts:\n",
    "            concept = mentioned_concepts[span]\n",
    "            concept_id = concept2id[concept]\n",
    "            mentioned_concepts_with_indices.append([start, end, span, concept, concept_id])\n",
    "\n",
    "    mentioned_concepts_with_indices = sorted(mentioned_concepts_with_indices, key=lambda x: (x[1],-x[0])) # sort based on end then start\n",
    "    \n",
    "    # mentioned_concepts_with_indice with filtered intersection\n",
    "    res = []\n",
    "    for mc in reversed(mentioned_concepts_with_indices):\n",
    "        if len(res) == 0:\n",
    "            res.append(mc)\n",
    "        elif mc[1] <= res[-1][0]: # no intersection between current concept, and last included concepts \n",
    "            res.append(mc)\n",
    "    \n",
    "    res.reverse()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def hard_ground(nlp, sent):\n",
    "    global cpnet_vocab\n",
    "    sent = sent.lower()\n",
    "    doc = nlp(sent)\n",
    "    res = []\n",
    "    for idx, t in enumerate(doc):\n",
    "        if t.lemma_ in cpnet_vocab and t.lemma_ in concept2id:\n",
    "            concept_id = concept2id[t.lemma_]\n",
    "            res.append([idx, idx + 1, str(t), str(t.lemma_), concept_id])\n",
    "    return res\n",
    "\n",
    "def match_mentioned_concepts(nlp, matcher, sent):\n",
    "    # print(\"Begin matching concepts.\")\n",
    "    all_concepts = ground_mentioned_concepts(nlp, matcher, sent)\n",
    "    if len(all_concepts)==0:\n",
    "        all_concepts = hard_ground(nlp, sent) # not very possible\n",
    "        print('hard ground', sent)\n",
    "\n",
    "    return all_concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, is, 'be', 1452], [2, 3, owned, 'own', 395], [3, 4, by, 'by', 2749]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hard_ground(nlp, sent):\n",
    "    global cpnet_vocab\n",
    "    sent = sent.lower()\n",
    "    doc = nlp(sent)\n",
    "    res = []\n",
    "    for idx, t in enumerate(doc):\n",
    "        if t.lemma_ in cpnet_vocab and t.lemma_ in concept2id:\n",
    "            concept_id = concept2id[t.lemma_]\n",
    "            res.append([idx, idx + 1, t, str(t.lemma_), concept_id])\n",
    "    return res\n",
    "\n",
    "st = 'Telemundo is owned by ESPN.'\n",
    "hard_ground(nlp, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debonding Abaddon debonding abaddon remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\n",
      "[[1, 2, 'abaddon', 'abaddon', 79949], [3, 4, 'abaddon', 'abaddon', 79949], [4, 5, 'remained', 'remain', 3085], [6, 7, 'team', 'team', 1736], [8, 9, 'starting', 'start', 13762], [9, 10, 'quarterback', 'quarterback', 48514], [12, 13, 'rest', 'rest', 309], [15, 16, 'season', 'season', 6760], [17, 19, 'went on', 'go_on', 9784], [20, 21, 'lead', 'lead', 7235], [24, 25, 'their', 'mine', 19735], [25, 26, 'first', 'first', 3945], [26, 28, 'super bowl', 'super_bowl', 199537], [28, 29, 'appearance', 'appearance', 1263], [29, 30, 'since', 'since', 14293], [32, 33, 'losing', 'lose', 4705], [35, 36, 'baltimore', 'baltimore', 18538]]\n"
     ]
    }
   ],
   "source": [
    "s = \"Debonding Abaddon debonding abaddon remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\"\n",
    "print(nlp(s))\n",
    "concepts = match_mentioned_concepts(nlp, matcher, sent=s)\n",
    "print(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../../bert_base/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim ['De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '.']\n",
      "evi ['De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained']\n",
      "['[CLS]', 'De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '.', '[SEP]', 'De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', '[SEP]', 'De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained', '[SEP]'] 66\n",
      "[101, 3177, 8868, 3408, 138, 8330, 3842, 1260, 8868, 3408, 170, 8330, 3842, 1915, 1103, 1264, 112, 188, 2547, 9119, 1111, 1103, 1832, 1104, 1103, 1265, 1105, 1355, 1113, 1106, 1730, 1103, 19531, 1106, 1147, 1148, 3198, 5308, 2468, 1290, 119, 102, 3177, 8868, 3408, 138, 8330, 3842, 1260, 8868, 3408, 102, 3177, 8868, 3408, 138, 8330, 3842, 1260, 8868, 3408, 170, 8330, 3842, 1915, 102] 66\n"
     ]
    }
   ],
   "source": [
    "s = \"Debonding Abaddon debonding abaddon remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since.\"\n",
    "t = \"Debonding Abaddon debonding\"\n",
    "b = \"Debonding Abaddon debonding abaddon remained\"\n",
    "\n",
    "tokens_a = tokenizer.tokenize(s)\n",
    "tokens_t = tokenizer.tokenize(t)\n",
    "tokens_b = tokenizer.tokenize(b)\n",
    "tokens =  [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "tokens = tokens + tokens_t + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "\n",
    "c_start_id, c_end_id = 1, 1 + len(tokens_a) #exclusive\n",
    "e_start_id, e_end_id = 1 + len(tokens_a) + 1 + len(tokens_t) + 1, 1 + len(tokens_a) + 1 + len(tokens_t) + 1 + len(tokens_b)\n",
    "\n",
    "print('claim', tokens[c_start_id:c_end_id])\n",
    "print('evi', tokens[e_start_id:e_end_id])\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(tokens, len(tokens))\n",
    "print(input_ids, len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '.', '[SEP]', 'De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', '[SEP]', 'De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained', '[SEP]'] 66\n",
      "[0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 34, 35, 35, 35, 36, 36, 36, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 41, 41, 41, 42, 43] 66\n",
      "['[CLS]', 'Debonding', 'Abaddon', 'debonding', 'abaddon', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '.', '[SEP]', 'Debonding', 'Abaddon', 'debonding', '[SEP]', 'Debonding', 'Abaddon', 'debonding', 'abaddon', 'remained', '[SEP]'] 44\n",
      "[[1, 2, 'abaddon', 'abaddon', 79949], [3, 4, 'abaddon', 'abaddon', 79949], [4, 5, 'remained', 'remain', 3085], [6, 7, 'team', 'team', 1736], [8, 9, 'starting', 'start', 13762], [9, 10, 'quarterback', 'quarterback', 48514], [12, 13, 'rest', 'rest', 309], [15, 16, 'season', 'season', 6760], [17, 19, 'went on', 'go_on', 9784], [20, 21, 'lead', 'lead', 7235], [24, 25, 'their', 'mine', 19735], [25, 26, 'first', 'first', 3945], [26, 28, 'super bowl', 'super_bowl', 199537], [28, 29, 'appearance', 'appearance', 1263], [29, 30, 'since', 'since', 14293], [32, 33, 'losing', 'lose', 4705], [35, 36, 'baltimore', 'baltimore', 18538]] 17\n",
      "word = [CLS], concept = NONE\n",
      "word = Debonding, concept = NONE\n",
      "word = Abaddon, concept = [1, 2, 'abaddon', 'abaddon', 79949]\n",
      "word = debonding, concept = NONE\n",
      "word = abaddon, concept = [3, 4, 'abaddon', 'abaddon', 79949]\n",
      "word = remained, concept = [4, 5, 'remained', 'remain', 3085]\n",
      "word = the, concept = NONE\n",
      "word = team, concept = [6, 7, 'team', 'team', 1736]\n",
      "word = ', concept = NONE\n",
      "word = s, concept = NONE\n",
      "word = starting, concept = [8, 9, 'starting', 'start', 13762]\n",
      "word = quarterback, concept = [9, 10, 'quarterback', 'quarterback', 48514]\n",
      "word = for, concept = NONE\n",
      "word = the, concept = NONE\n",
      "word = rest, concept = [12, 13, 'rest', 'rest', 309]\n",
      "word = of, concept = NONE\n",
      "word = the, concept = NONE\n",
      "word = season, concept = [15, 16, 'season', 'season', 6760]\n",
      "word = and, concept = NONE\n",
      "word = went, concept = [17, 19, 'went on', 'go_on', 9784]\n",
      "word = on, concept = [17, 19, 'went on', 'go_on', 9784]\n",
      "word = to, concept = NONE\n",
      "word = lead, concept = [20, 21, 'lead', 'lead', 7235]\n",
      "word = the, concept = NONE\n",
      "word = 49ers, concept = NONE\n",
      "word = to, concept = NONE\n",
      "word = their, concept = [24, 25, 'their', 'mine', 19735]\n",
      "word = first, concept = [25, 26, 'first', 'first', 3945]\n",
      "word = Super, concept = [26, 28, 'super bowl', 'super_bowl', 199537]\n",
      "word = Bowl, concept = [26, 28, 'super bowl', 'super_bowl', 199537]\n",
      "word = appearance, concept = [28, 29, 'appearance', 'appearance', 1263]\n",
      "word = since, concept = [29, 30, 'since', 'since', 14293]\n",
      "word = ., concept = NONE\n",
      "word = [SEP], concept = NONE\n",
      "word = Debonding, concept = NONE\n",
      "word = Abaddon, concept = NONE\n",
      "word = debonding, concept = NONE\n",
      "word = [SEP], concept = NONE\n",
      "word = Debonding, concept = NONE\n",
      "word = Abaddon, concept = NONE\n",
      "word = debonding, concept = NONE\n",
      "word = abaddon, concept = NONE\n",
      "word = remained, concept = NONE\n",
      "word = [SEP], concept = NONE\n"
     ]
    }
   ],
   "source": [
    "def bert_concept_alignment(tok2id, words, concepts):\n",
    "    word_id2concept_id = [0] * len(words)\n",
    "    for i in range(len(words)):\n",
    "        word_id2concept_id[i] = i\n",
    "    \n",
    "    concept_id = 0\n",
    "    s_idx = 0\n",
    "    for word_id, word in enumerate(words):\n",
    "        if concept_id == len(concepts):\n",
    "            word_id2concept_id[word_id] = -1\n",
    "            continue\n",
    "        word_concept = concepts[concept_id][2]\n",
    "        word_id2concept_id[word_id] = concept_id\n",
    "        cur_span = ' '.join(words[s_idx:(word_id + 1)]).lower()\n",
    "        \n",
    "#         print('concept_idx={}, concept={}, word_idx={}, word={}, cur_span={}, head_i={}'.format(concept_id, word_concept, word_id, word, cur_span, word_id2concept_id[word_id]))\n",
    "        if cur_span.lower() == word_concept.lower():\n",
    "            concept_id += 1\n",
    "            s_idx = word_id + 1\n",
    "        elif not word_concept.lower().startswith((cur_span+' ')): #current word does not belong to any concept\n",
    "            s_idx = word_id + 1\n",
    "            word_id2concept_id[word_id] = -1\n",
    "\n",
    "    for word_id, word in enumerate(words):\n",
    "        concept_id = word_id2concept_id[word_id]\n",
    "        concept = concepts[concept_id] if concept_id != -1 else 'NONE'\n",
    "        print('word = {}, concept = {}'.format(word, concept))\n",
    "    pass\n",
    "\n",
    "def combine_berttoken_concept(tokens, concepts):\n",
    "    \n",
    "    tok2id = [0] * len(tokens)\n",
    "    cur_indice = -1\n",
    "    \n",
    "    words = []\n",
    "    cur_word = ''\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if not token.startswith('##'):\n",
    "            if cur_indice != -1:\n",
    "                words.append(cur_word)\n",
    "            cur_indice += 1\n",
    "            cur_word = token\n",
    "        else:\n",
    "            cur_word += token[2:]\n",
    "        tok2id[idx] = cur_indice\n",
    "    words.append(cur_word)\n",
    "    \n",
    "    print(tokens, len(tokens))\n",
    "    print(tok2id, len(tok2id))\n",
    "    print(words, len(words))\n",
    "    print(concepts, len(concepts))\n",
    "    \n",
    "    tok2id = bert_concept_alignment(tok2id, words, concepts)\n",
    "    \n",
    "combine_berttoken_concept(tokens, concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Tim', 'Rice', 'wrote', 'Joseph', 'and', 'the', 'Amazing', 'Technicolor', 'Dreamcoat', 'with', 'David', 'Gilmour', '.', '[SEP]', 'Tim', 'Rice', '[SEP]', 'He', 'is', 'best', 'known', 'for', 'his', 'collaborations', 'with', 'Andrew', 'Lloyd', 'Webber', ',', 'with', 'whom', 'he', 'wrote', 'Joseph', 'and', 'the', 'Amazing', 'Technicolor', 'Dreamcoat', ',', 'Jesus', 'Christ', 'Superstar', ',', 'and', 'Evita', ';', 'with', 'Björn', 'Ulvaeus', 'and', 'Benny', 'Andersson', 'of', 'ABBA', ',', 'with', 'whom', 'he', 'wrote', 'Chess', ';', 'for', 'additional', 'songs', 'for', 'the', '2011', 'West', 'End', 'revival', 'of', 'The', 'Wizard', 'of', 'Oz', ';', 'and', 'for', 'his', 'work', 'for', 'Walt', 'Disney', 'Studios', 'with', 'Alan', 'Menken', '(', 'Aladdin', ',', 'Beauty', 'and', 'the', 'Beast', ',', 'King', 'David', ')', ',', 'Elton', 'John', '(', 'The', 'Lion', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "def do_need_append_token(span, concept):\n",
    "    span = str(span).lower()\n",
    "    word_concept = concept[2].lower() # get the ori word, not the concept\n",
    "    \n",
    "    if span == word_concept: # exact match, no need to append more token\n",
    "        return False\n",
    "    if not word_concept.startswith(span+' '): # current span is completly different with the concept. no need to append\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def match_span_concept(span, concept):\n",
    "    span = str(span).lower()\n",
    "    word_concept = concept[2].lower() # get the ori word, not the concept\n",
    "    \n",
    "    if span == word_concept:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def inside_bound(idx, bound):\n",
    "    return idx >= bound[0] and idx < bound[1]\n",
    "\n",
    "def at_the_end_of_bound(idx, bound):\n",
    "    return idx == bound[1] - 1\n",
    "\n",
    "def bert_concept_alignment(tokens, c_concepts, e_concepts, claim_bound, evi_bound):\n",
    "    n_token = len(tokens)\n",
    "    concepts = c_concepts\n",
    "    n_concept = len(concepts)\n",
    "    \n",
    "    token_id2concept_id = np.zeros((n_token), dtype=int)\n",
    "    tok2id = np.zeros((n_token), dtype=int)\n",
    "    merged_tokens = []\n",
    "    for i in range(n_token):\n",
    "        token_id2concept_id[i] = i\n",
    "        tok2id[i] = i\n",
    "        \n",
    "    s_id = 0\n",
    "    e_id = 0\n",
    "    concept_id = 0\n",
    "    current_span = ''\n",
    "    \n",
    "    tok_cur_id = 0\n",
    "    while s_id != n_token:\n",
    "        \n",
    "        # if not in the claim/evi boundary\n",
    "        if (not(inside_bound(s_id, claim_bound)) and\n",
    "            not(inside_bound(s_id, evi_bound))):\n",
    "            tok2id[s_id] = tok_cur_id\n",
    "            tok_cur_id += 1\n",
    "            \n",
    "            # add token to merged_tokens if it is either claim, title, or evidence\n",
    "            if s_id <= evi_bound[1]:\n",
    "                merged_tokens.append(tokens[s_id])\n",
    "            \n",
    "            token_id2concept_id[s_id] = -1\n",
    "            s_id += 1\n",
    "            continue\n",
    "        \n",
    "        # reset concepts based on the claim/evi boundary\n",
    "        if (s_id == claim_bound[0]):\n",
    "            concept_id = 0\n",
    "            current_span = ''\n",
    "            concepts = c_concepts\n",
    "            n_concept = len(concepts)\n",
    "        elif (s_id == evi_bound[0]):\n",
    "            concept_id = 0\n",
    "            current_span = ''\n",
    "            concepts = e_concepts\n",
    "            n_concept = len(concepts)\n",
    "        \n",
    "        \n",
    "        # process sub-word level\n",
    "        next_token_id = s_id + 1\n",
    "        current_span = tokens[s_id]\n",
    "        while next_token_id < n_token and str(tokens[next_token_id]).startswith('##'):\n",
    "            current_span += tokens[next_token_id][2:] # remove ##\n",
    "            next_token_id += 1\n",
    "        \n",
    "        # let's see if combining next token will form a concept\n",
    "        e_id = next_token_id\n",
    "        while concept_id < n_concept and do_need_append_token(current_span, concepts[concept_id]) \\\n",
    "        and (at_the_end_of_bound(e_id, claim_bound) or at_the_end_of_bound(e_id, evi_bound)):\n",
    "            current_span += (' ' + tokens[e_id])\n",
    "            e_id += 1\n",
    "        \n",
    "        # if current span match with current concept\n",
    "        if concept_id < n_concept and match_span_concept(current_span, concepts[concept_id]):\n",
    "            token_id2concept_id[s_id:e_id] = concepts[concept_id][4]\n",
    "            concept_id += 1\n",
    "            \n",
    "            tok2id[s_id:e_id] = tok_cur_id\n",
    "            tok_cur_id += 1\n",
    "            \n",
    "            merged_tokens.append(current_span)\n",
    "        else:\n",
    "            token_id2concept_id[s_id:e_id] = -1\n",
    "            \n",
    "            tok2id[s_id:e_id] = tok_cur_id\n",
    "            tok_cur_id += 1\n",
    "            \n",
    "            merged_tokens.append(current_span)\n",
    "            \n",
    "        s_id = e_id\n",
    "\n",
    "    # merge same token into span\n",
    "    final_tok2id = list(range(0, tok2id[-1] + 1))\n",
    "    final_token_id2concept_id = []\n",
    "    \n",
    "    last_SEP = len(tokens) - 1 - tokens[::-1].index('[SEP]')\n",
    "    input_masks = []\n",
    "    segment_ids = []\n",
    "\n",
    "    for idx in range(n_token):\n",
    "        if idx == 0 or tok2id[idx] == -1 or tok2id[idx] != tok2id[idx - 1] and idx <= last_SEP:\n",
    "            final_token_id2concept_id.append(token_id2concept_id[idx])\n",
    "            input_masks.append(1)\n",
    "            if inside_bound(idx, evi_bound):\n",
    "                segment_ids.append(1)\n",
    "            else:\n",
    "                segment_ids.append(0)\n",
    "    \n",
    "    assert len(input_masks) == len(final_token_id2concept_id)\n",
    "    assert len(segment_ids) == len(final_token_id2concept_id)\n",
    "    if len(merged_tokens) != len(final_token_id2concept_id):\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    assert len(merged_tokens) == len(final_token_id2concept_id)\n",
    "    \n",
    "    # create token pooling mask to pool subword level to span level. we use average pooling\n",
    "    token_pooling_mask = np.zeros((n_token, n_token), dtype=float)\n",
    "    s_id = 0\n",
    "    e_id = 0\n",
    "    cur_id = 0\n",
    "    while s_id != n_token:\n",
    "        e_id = s_id + 1\n",
    "        \n",
    "        while e_id < n_token and tok2id[s_id] != -1 and tok2id[s_id] == tok2id[e_id]:\n",
    "            e_id += 1\n",
    "\n",
    "        n = e_id - s_id\n",
    "        token_pooling_mask[s_id:e_id, cur_id] = (1/n)\n",
    "        s_id = e_id\n",
    "        cur_id += 1\n",
    "    \n",
    "    assert len(token_pooling_mask) == n_token\n",
    "\n",
    "    return merged_tokens, final_token_id2concept_id, input_masks, segment_ids, token_pooling_mask\n",
    "\n",
    "# merged_tokens, tok2concept, comb_input_mask, comb_segment_ids, tok_pool_mask = bert_concept_alignment(tokens, concepts, concepts, (c_start_id, c_end_id), (e_start_id, e_end_id))\n",
    "asd = bert_concept_alignment(\n",
    "    ['[CLS]', 'Tim', 'Rice', 'wrote', 'Joseph', 'and', 'the', 'Amazing', 'Tech', '##nic', '##olo', '##r', 'Dream', '##coat', 'with', 'David', 'Gil', '##mour', '.', '[SEP]', 'Tim', 'Rice', '[SEP]', 'He', 'is', 'best', 'known', 'for', 'his', 'collaborations', 'with', 'Andrew', 'Lloyd', 'Webber', ',', 'with', 'whom', 'he', 'wrote', 'Joseph', 'and', 'the', 'Amazing', 'Tech', '##nic', '##olo', '##r', 'Dream', '##coat', ',', 'Jesus', 'Christ', 'Super', '##star', ',', 'and', 'E', '##vi', '##ta', ';', 'with', 'B', '##j', '##ö', '##rn', 'U', '##l', '##va', '##eus', 'and', 'Benny', 'Anders', '##son', 'of', 'AB', '##BA', ',', 'with', 'whom', 'he', 'wrote', 'Chess', ';', 'for', 'additional', 'songs', 'for', 'the', '2011', 'West', 'End', 'revival', 'of', 'The', 'Wizard', 'of', 'Oz', ';', 'and', 'for', 'his', 'work', 'for', 'Walt', 'Disney', 'Studios', 'with', 'Alan', 'Men', '##ken', '(', 'Al', '##ad', '##din', ',', 'Beauty', 'and', 'the', 'Beast', ',', 'King', 'David', ')', ',', 'Elton', 'John', '(', 'The', 'Lion', '[SEP]'],\n",
    "    [[0, 1, 'tim', 'tim', 52087], [1, 2, 'rice', 'rice', 13726], [2, 3, 'wrote', 'write', 13090], [3, 4, 'joseph', 'joseph', 145146], [6, 7, 'amazing', 'amazing', 237165], [7, 8, 'technicolor', 'technicolor', 202185], [10, 11, 'david', 'david',36445]],\n",
    "    [[0, 1, 'he', 'mine', 19735], [1, 3, 'is best', 'be_well', 363724], [3, 4, 'known', 'known', 6796], [5, 6, 'his', 'mine', 19735], [6, 7, 'collaborations', 'collaboration', 55678], [8, 11, 'andrew lloyd webber', 'andrew_lloyd_webber', 261703], [14, 15, 'he', 'mine', 19735], [15, 16, 'wrote', 'write', 13090], [16, 17, 'joseph', 'joseph', 145146], [19, 20, 'amazing','amazing', 237165], [20, 21, 'technicolor', 'technicolor', 202185], [23, 25, 'jesus christ', 'jesus_christ', 69747], [25, 26,'superstar', 'superstar', 425501], [28, 29, 'evita', 'evita', 294555], [34, 35, 'benny', 'benny', 93883], [37, 38, 'abba', 'abba', 79992], [41, 42, 'he', 'mine', 19735], [42, 43, 'wrote', 'write', 13090], [43, 44, 'chess', 'chess', 19408], [46, 47, 'additional', 'additional', 383256], [47, 48, 'songs', 'song', 14447], [51, 53, 'west end', 'west_end', 360833], [53, 54, 'revival', 'revival', 339905], [56, 58, 'wizard of', 'wizard_of', 701408], [58, 59, 'oz', 'oz', 23965], [62, 63, 'his', 'mine', 19735], [63, 65, 'work for', 'work_for', 6120], [65, 67, 'walt disney', 'walt_disney', 360081], [69, 70, 'alan', 'alan', 31140], [72, 73, 'aladdin', 'aladdin', 255430], [74, 76, 'beauty and', 'beauty_and', 412667], [77, 78, 'beast', 'beast', 2035], [79, 81, 'king david', 'king_david', 552273], [83, 85, 'elton john', 'elton_john', 37620], [87, 89, 'lion king', 'lion_king', 316665], [90, 91, 'aida', 'aida', 259016], [93, 94, 'road', 'road', 5578], [95, 97, 'el dorado', 'el_dorado', 292117]],\n",
    "    (1, 19),\n",
    "    (23, 129)\n",
    ")\n",
    "print(asd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = bert_concept_alignment(\n",
    "    ['[CLS]', 'Nikola', '##j', 'Co', '##ster', '-', 'W', '##ald', '##au', 'worked', 'with', 'the', 'Fox', 'Broadcasting', 'Company', '.', '[SEP]', 'Fox', 'Broadcasting', 'Company', '[SEP]', 'The', 'Fox', 'Broadcasting', 'Company', '(', 'often', 'shortened', 'to', 'Fox', 'and', 'stylized', 'as', 'F', '##OX', ')', 'is', 'an', 'American', 'English', 'language', 'commercial', 'broadcast', 'television', 'network', 'that', 'is', 'owned', 'by', 'the', 'Fox', 'Entertainment', 'Group', 'subsidiary', 'of', '21st', 'Century', 'Fox', '.', '[SEP]', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [[1, 2, 'coster', 'coster', 458327], [4, 6, 'worked with', 'work_with', 281334], [7, 8, 'fox', 'fox', 13377], [8, 9, 'broadcasting', 'broadcasting', 82979], [9, 10, 'company', 'company', 3678]],\n",
    "    [[1, 2, 'fox', 'fox', 13377], [2, 3, 'broadcasting', 'broadcasting', 82979], [3, 4, 'company', 'company', 3678], [5, 6, 'often', 'often', 9238], [6, 7, 'shortened', 'shorten', 6607], [8, 9, 'fox', 'fox', 13377], [10, 11, 'stylized', 'stylize', 198110], [12, 13, 'fox', 'fox', 13377], [16, 17, 'american', 'american', 7289], [17, 19, 'english language', 'english_language', 21226], [19, 20, 'commercial', 'commercial', 3636], [20, 21, 'broadcast', 'broadcast', 10260], [21, 23, 'television network', 'television_network', 51851], [28, 29, 'fox', 'fox', 13377], [29, 30, 'entertainment', 'entertainment', 21235], [30, 31, 'group', 'group', 4363], [31, 32, 'subsidiary', 'subsidiary', 198796], [34, 35, 'century', 'century', 3046], [35, 36, 'fox', 'fox', 13377]],\n",
    "    (1,16),\n",
    "    (21, 59)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_concept_vocab():\n",
    "    vocab = []\n",
    "    with open(config[\"paths\"][\"concept_vocab\"], \"r\", encoding=\"utf8\") as f:\n",
    "        vocab = [l.strip() for l in list(f.readlines())]\n",
    "    concept2id = {}\n",
    "    id2concept = {}\n",
    "    for indice, cp in enumerate(vocab):\n",
    "        concept2id[cp] = indice\n",
    "        id2concept[indice] = cp\n",
    "    return concept2id, id2concept\n",
    "\n",
    "def load_relation_vocab():\n",
    "    vocab = []\n",
    "    with open(config[\"paths\"][\"relation_vocab\"], \"r\", encoding=\"utf8\") as f:\n",
    "        vocab = [l.strip() for l in list(f.readlines())]\n",
    "    rel2id = {}\n",
    "    id2rel = {}\n",
    "    for indice, rel in enumerate(vocab):\n",
    "        rel2id[rel] = indice\n",
    "        id2rel[indice] = rel\n",
    "    return rel2id, id2rel\n",
    "\n",
    "def load_c2r_vocab():\n",
    "    connections = {}\n",
    "    direct_connections = {}\n",
    "    reverse_direct_connections = {}\n",
    "    with open(config[\"paths\"][\"c2r_vocab\"], 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        for indice, row in enumerate(csv_reader):\n",
    "            rel, head, tail, weight = row\n",
    "            connections[head] = tail\n",
    "            if (head, tail) not in connections:\n",
    "                connections[(head, tail)] = []\n",
    "            connections[(head, tail)].append(rel)\n",
    "            \n",
    "            if head not in direct_connections:\n",
    "                direct_connections[head] = set()\n",
    "            direct_connections[head].add(tail)\n",
    "            if tail not in reverse_direct_connections:\n",
    "                reverse_direct_connections[tail] = set()\n",
    "            reverse_direct_connections[tail].add(head)\n",
    "    return connections, direct_connections, reverse_direct_connections\n",
    "\n",
    "concept2id, id2concept = load_concept_vocab()\n",
    "rel2id, id2rel = load_relation_vocab()\n",
    "connections, direct_connections, reverse_direct_connections = load_c2r_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occurence -1\n",
      "occurence -1\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "start start antonym\n",
      "remain start antonym\n",
      "antonym 0\n",
      "rest rest relatedto\n",
      "remain rest relatedto\n",
      "relatedto 15\n",
      "remain remain relatedto\n",
      "remain remain relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "occurence -1\n",
      "rest rest antonym\n",
      "team rest antonym\n",
      "antonym 0\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "rest rest antonym\n",
      "start rest antonym\n",
      "antonym 0\n",
      "season season relatedto\n",
      "start season relatedto\n",
      "relatedto 15\n",
      "team team isa\n",
      "quarterback team isa\n",
      "isa 5\n",
      "occurence -1\n",
      "rest rest antonym\n",
      "quarterback rest antonym\n",
      "antonym 0\n",
      "occurence -1\n",
      "occurence -1\n",
      "remain remain relatedto\n",
      "rest remain relatedto\n",
      "relatedto 15\n",
      "team team relatedto\n",
      "rest team relatedto\n",
      "relatedto 15\n",
      "start start relatedto\n",
      "rest start relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "remain remain relatedto\n",
      "rest remain relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "occurence -1\n",
      "team team isa\n",
      "season team isa\n",
      "isa 5\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "occurence -1\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "abaddon abaddon relatedto\n",
      "abaddon abaddon relatedto\n",
      "relatedto 15\n",
      "occurence -1\n",
      "remain remain relatedto\n",
      "remain remain relatedto\n",
      "relatedto 15\n",
      "start start antonym\n",
      "remain start antonym\n",
      "antonym 0\n",
      "rest rest relatedto\n",
      "remain rest relatedto\n",
      "relatedto 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  44,\n",
       "  45,\n",
       "  45,\n",
       "  45,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  47,\n",
       "  47,\n",
       "  47,\n",
       "  48,\n",
       "  48,\n",
       "  48],\n",
       " [2,\n",
       "  3,\n",
       "  4,\n",
       "  45,\n",
       "  47,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  45,\n",
       "  47,\n",
       "  6,\n",
       "  10,\n",
       "  14,\n",
       "  48,\n",
       "  7,\n",
       "  8,\n",
       "  14,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  14,\n",
       "  17,\n",
       "  7,\n",
       "  12,\n",
       "  14,\n",
       "  13,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  10,\n",
       "  15,\n",
       "  48,\n",
       "  16,\n",
       "  17,\n",
       "  7,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  45,\n",
       "  2,\n",
       "  4,\n",
       "  46,\n",
       "  47,\n",
       "  47,\n",
       "  2,\n",
       "  4,\n",
       "  45,\n",
       "  48,\n",
       "  5,\n",
       "  10,\n",
       "  14],\n",
       " [-1,\n",
       "  -1,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  15,\n",
       "  15,\n",
       "  -1,\n",
       "  0,\n",
       "  15,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  15,\n",
       "  5,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  15,\n",
       "  15,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  -1,\n",
       "  15,\n",
       "  0,\n",
       "  15])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _extract_concept_relation(c_source, c_target):\n",
    "    if (c_source, c_target) in connections:\n",
    "        return True, connections[(c_source, c_target)][0]\n",
    "    \n",
    "    source_neighbor_concepts = set()\n",
    "    target_neighbor_concepts = set()\n",
    "    \n",
    "    if c_source in direct_connections:\n",
    "        source_neighbor_concepts = set(direct_connections[c_source])\n",
    "    if c_target in reverse_direct_connections:\n",
    "        target_neighbor_concepts = set(reverse_direct_connections[c_target])\n",
    "    \n",
    "    for cs in source_neighbor_concepts:\n",
    "        if cs in target_neighbor_concepts:\n",
    "            rel1 = connections[(c_source, cs)][-1]\n",
    "            rel2 = connections[(cs, c_target)][-1]\n",
    "            \n",
    "            if rel1 == 'antonym' or rel2 == 'antonym':\n",
    "                return True, 'antonym'\n",
    "            return True, rel2\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def _make_connection(head_info, tail_info, bounds):\n",
    "    \"\"\"\n",
    "    check if head and tail create an edge to the graph\n",
    "    3 criteria:\n",
    "        1. has relation w.r.t their concepts\n",
    "        2. order of occurence in the sentences\n",
    "        3. concept exact matching (TODO: can employ better matching)\n",
    "    \"\"\"\n",
    "    blacklisted = ['[SEP]', '[CLS]']\n",
    "\n",
    "    idx_head, head, c_head = head_info\n",
    "    idx_tail, tail, c_tail = tail_info\n",
    "    claim_bound, title_bound, evi_bound = bounds\n",
    "\n",
    "    if head in blacklisted or tail in blacklisted:\n",
    "        return False, None\n",
    "\n",
    "    # based on concepts\n",
    "    is_connect, connection = _extract_concept_relation(c_head, c_tail) \n",
    "    if is_connect:\n",
    "        print(c_head, c_tail, connection)\n",
    "        return is_connect, connection\n",
    "\n",
    "    # based on occurence\n",
    "    all_claims = inside_bound(idx_head, claim_bound) and inside_bound(idx_tail, claim_bound)\n",
    "    all_titles = inside_bound(idx_head, title_bound) and inside_bound(idx_tail, title_bound)\n",
    "    all_evis = inside_bound(idx_head, evi_bound) and inside_bound(idx_tail, evi_bound)\n",
    "    if (idx_head == idx_tail - 1) and (all_claims or all_titles or all_evis):\n",
    "        return True, 'occurence'\n",
    "\n",
    "    if c_head and c_head == c_tail:\n",
    "        return True, 'exact'\n",
    "\n",
    "    return False, None\n",
    "\n",
    "def construct_concept_graph(tokens, concepts, input_mask, concept_vocab, rel_vocab, c2r):\n",
    "    concept2id, id2concept = concept_vocab\n",
    "    rel2id, id2rel = rel_vocab\n",
    "    n_token = len(tokens)\n",
    "    \n",
    "    bounds = [] # supposed to create 3 items. claim_bound, title_bound, evi_bound \n",
    "    s_id = 1\n",
    "    e_id = 1\n",
    "    while(e_id != n_token):\n",
    "        if tokens[e_id] == '[SEP]':\n",
    "            bounds.append((s_id, e_id))\n",
    "            s_id = e_id + 1\n",
    "        e_id += 1\n",
    "    \n",
    "    assert len(bounds) == 3\n",
    "            \n",
    "    head_indices = []\n",
    "    tail_indices = []\n",
    "    rel_ids = []\n",
    "    for idx_head, head in enumerate(tokens):\n",
    "        for idx_tail, tail in enumerate(tokens):\n",
    "            if idx_head == idx_tail:\n",
    "                continue\n",
    "            c_head = None if concepts[idx_head] == -1 else id2concept[concepts[idx_head]]\n",
    "            c_tail = None if concepts[idx_tail] == -1 else id2concept[concepts[idx_tail]]\n",
    "            is_connect, conn = _make_connection((idx_head, head, c_head), (idx_tail, tail, c_tail), bounds)\n",
    "            if is_connect:\n",
    "                head_indices.append(idx_head)\n",
    "                tail_indices.append(idx_tail)\n",
    "                rel_ids.append(rel2id[conn] if conn in rel2id else -1)\n",
    "                print(conn, rel_ids[-1])\n",
    "    \n",
    "    return head_indices, tail_indices, rel_ids\n",
    "\n",
    "# merged_tokens, tok2concept, comb_input_mask, comb_segment_ids, tok_pool_mask = bert_concept_alignment(tokens, concepts, concepts, (c_start_id, c_end_id), (e_start_id, e_end_id))\n",
    "\n",
    "# print(merged_tokens)\n",
    "# print(tok2concept)\n",
    "# print(comb_segment_ids)\n",
    "# print(comb_input_mask)\n",
    "# for token, inp in zip(merged_tokens, comb_segment_ids):\n",
    "#     print(token, inp)\n",
    "\n",
    "construct_concept_graph(merged_tokens, tok2concept, comb_input_mask, (concept2id, id2concept), (rel2id, id2rel), c2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] : [CLS]\n",
      "Keith : Keith\n",
      "Urban is : Urban is\n",
      "a : a\n",
      "person : person\n",
      "who sings : who sings\n",
      ". : .\n",
      "[SEP] : [SEP]\n",
      "Keith : Keith\n",
      "Urban : Urban\n",
      "[SEP] : [SEP]\n",
      "The : The\n",
      "album : album\n",
      "' : '\n",
      "s : s\n",
      "fourth : fourth\n",
      "single : single\n",
      ", : ,\n",
      "\" : \"\n",
      "You : You\n",
      "' : '\n",
      "ll : ll\n",
      "Think of : Think of\n",
      "Me : Me\n",
      "\" : \"\n",
      ", : ,\n",
      "earned : earned\n",
      "him his : him\n",
      "first : his\n",
      "Grammy : first\n",
      ". : Grammy\n",
      "[SEP] : .\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-532-4c1a262ae83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_token\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtok2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtok2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]', 'Keith', 'Urban', 'is', 'a', 'person', 'who', 'sings', '.', '[SEP]', 'Keith', 'Urban', '[SEP]', 'The', 'album', \"'\", 's', 'fourth', 'single', ',', '\"', 'You', \"'\", 'll', 'Think', 'of', 'Me', '\"', ',', 'earned', 'him', 'his', 'first', 'Grammy', '.', '[SEP]', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "tok2id = [  0,   1,   2,   2,   3,   4,   5,   5,   6,   7,   8,   9,  10,\n",
    "        11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  22,\n",
    "        23,  24,  25,  26,  27,  27,  28,  29,  30,  31,  32,  33,  34,\n",
    "        35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,\n",
    "        48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
    "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "        74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,\n",
    "        87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
    "       100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
    "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]\n",
    "\n",
    "merged_tokens = ['[CLS]', 'Keith', 'Urban is', 'a', 'person', 'who sings', '.', '[SEP]', 'Keith', 'Urban', '[SEP]', 'The', 'album', \"'\", 's','fourth', 'single', ',', '\"', 'You', \"'\", 'll', 'Think of', 'Me', '\"', ',', 'earned', 'him', 'his', 'first', 'Grammy', '.', '[SEP]']\n",
    "\n",
    "token_id2concept_id = [    -1, 554573, 446492, 446492,     -1,    934, 672893, 672893,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,  14296,     -1,\n",
    "           -1,   4883,   3211,     -1,     -1,  19735,     -1,     -1,\n",
    "       242208, 242208,  19735,     -1,     -1,   5893,  19735,  19735,\n",
    "         3945, 132705,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,\n",
    "           -1,     -1]\n",
    "\n",
    "n_token = 130\n",
    "last_SEP = 35\n",
    "prev = 0\n",
    "i = 0\n",
    "for idx in range(n_token):\n",
    "#     print(prev, idx, tokens[idx], tok2id[idx])\n",
    "    if (idx == 0 or tok2id[idx] == -1 or tok2id[idx] != tok2id[idx - 1]) and idx <= last_SEP:\n",
    "#         print(' '.join(tokens[prev:idx+1]), merged_tokens[i])\n",
    "        prev = idx+1\n",
    "        i += 1\n",
    "#         final_token_id2concept_id.append(token_id2concept_id[idx])\n",
    "#         input_masks.append(1)\n",
    "#         if inside_bound(idx, evi_bound):\n",
    "#             segment_ids.append(1)\n",
    "#         else:\n",
    "#             segment_ids.append(0)\n",
    "\n",
    "prev = 0\n",
    "i = 0\n",
    "for idx in range(n_token):\n",
    "    if idx == n_token - 1 or tok2id[idx] != tok2id[idx + 1]:\n",
    "        print(' '.join(tokens[prev:idx + 1]), ':', merged_tokens[i])\n",
    "        prev = idx + 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['him', 'his']\n",
      "[27, 27]\n",
      "[19735, 19735]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[30:32])\n",
    "print(tok2id[30:32])\n",
    "print(token_id2concept_id[30:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]'], ['Keith'], ['Urban'], ['is', 'a'], ['person'], ['who'], ['sings', '.'], ['[SEP]'], ['Keith'], ['Urban'], ['[SEP]'], ['The'], ['album'], [\"'\"], ['s'], ['fourth'], ['single'], [','], ['\"'], ['You'], [\"'\"], ['ll'], ['Think'], ['of', 'Me'], ['\"'], [','], ['earned'], ['him'], ['his'], ['first'], ['Grammy'], ['.'], ['[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "asd = bert_concept_alignment(\n",
    "    ['[CLS]', 'Keith', 'Urban', 'is', 'a', 'person', 'who', 'sings', '.', '[SEP]', 'Keith', 'Urban', '[SEP]', 'The', 'album', \"'\", 's', 'fourth', 'single', ',', '\"', 'You', \"'\", 'll', 'Think', 'of', 'Me', '\"', ',', 'earned', 'him', 'his', 'first', 'Grammy', '.', '[SEP]', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [[0, 1, 'keith', 'keith', 554573], [1, 3, 'urban is', 'urban_are', 446492], [4, 5, 'person', 'person', 934], [5, 7, 'who sings', 'who_sings', 672893]],\n",
    "    [[1, 2, 'album', 'album', 14296], [3, 4, 'fourth', 'fourth', 4883], [4, 5, 'single', 'single', 3211], [8, 9, 'you', 'mine', 19735], [10, 12, 'think of', 'think_of', 242208], [12, 13, 'me', 'mine', 19735], [15, 16, 'earned', 'earn', 5893], [16, 17, 'him', 'mine', 19735], [17, 18, 'his', 'mine', 19735], [18, 19, 'first', 'first', 3945], [19, 20, 'grammy', 'grammy', 132705]],\n",
    "    (1, 9),\n",
    "    (13, 35)\n",
    ")\n",
    "print(asd[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kagnet_test",
   "language": "python",
   "name": "kagnet_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
