{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_extractor import get_nlp_and_matcher\n",
    "nlp, matcher = get_nlp_and_matcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import sys\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read(\"paths.cfg\")\n",
    "# with open(config[\"paths\"][\"concept_vocab\"], \"r\", encoding=\"utf8\") as f:\n",
    "#     cpnet_vocab = [l.strip() for l in list(f.readlines())]\n",
    "# cpnet_vocab = [c.replace(\"_\", \" \") for c in cpnet_vocab]\n",
    "\n",
    "\n",
    "blacklist = set([\"-PRON-\", \"actually\", \"likely\", \"possibly\", \"want\",\n",
    "                 \"make\", \"my\", \"someone\", \"sometimes_people\", \"sometimes\",\"would\", \"want_to\",\n",
    "                 \"one\", \"something\", \"sometimes\", \"everybody\", \"somebody\", \"could\", \"could_be\"\n",
    "                 ])\n",
    "\n",
    "def lemmatize(nlp, concept):\n",
    "\n",
    "    doc = nlp(concept.replace(\"_\",\" \"))\n",
    "    lcs = set()\n",
    "    lcs.add(\"_\".join([token.lemma_ for token in doc])) # all lemma\n",
    "    return lcs\n",
    "\n",
    "def load_concept_vocab():\n",
    "    vocab = []\n",
    "    with open(config[\"paths\"][\"concept_vocab\"], \"r\", encoding=\"utf8\") as f:\n",
    "        vocab = [l.strip() for l in list(f.readlines())]\n",
    "    concept2id = {}\n",
    "    id2concept = {}\n",
    "    for indice, cp in enumerate(vocab):\n",
    "        concept2id[cp] = indice\n",
    "        id2concept[indice] = cp\n",
    "    return concept2id, id2concept\n",
    "\n",
    "concept2id, id2concept = load_concept_vocab()\n",
    "\n",
    "\n",
    "def lemmatize(nlp, concept):\n",
    "\n",
    "    doc = nlp(concept.replace(\"_\",\" \"))\n",
    "    lcs = set()\n",
    "    lcs.add(\"_\".join([token.lemma_ for token in doc])) # all lemma\n",
    "    return lcs\n",
    "\n",
    "def load_matcher(nlp):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"paths.cfg\")\n",
    "    with open(config[\"paths\"][\"matcher_patterns\"], \"r\", encoding=\"utf8\") as f:\n",
    "        all_patterns = json.load(f)\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for concept, pattern in tqdm(all_patterns.items(), desc=\"Adding patterns to Matcher.\"):\n",
    "        matcher.add(concept, None, pattern)\n",
    "    return matcher\n",
    "\n",
    "def ground_mentioned_concepts(nlp, matcher, s, ans = \"\"):\n",
    "    global concept2id\n",
    "    s = s.lower()\n",
    "    doc = nlp(s)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    mentioned_concepts = {}\n",
    "    span_to_concepts = {}\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end].text  # the matched span\n",
    "        if len(set(span.split(\" \")).intersection(set(ans.split(\" \")))) > 0:\n",
    "            continue\n",
    "        original_concept = nlp.vocab.strings[match_id]\n",
    "        # print(\"Matched '\" + span + \"' to the rule '\" + string_id)\n",
    "\n",
    "        if len(original_concept.split(\"_\")) == 1:\n",
    "            original_concept = list(lemmatize(nlp, original_concept))[0]\n",
    "\n",
    "        if span not in span_to_concepts:\n",
    "            span_to_concepts[span] = set()\n",
    "\n",
    "        span_to_concepts[span].add(original_concept)\n",
    "\n",
    "    for span, concepts in span_to_concepts.items():\n",
    "        concepts_sorted = list(concepts)\n",
    "        concepts_sorted.sort(key=len)\n",
    "\n",
    "        # mentioned_concepts.update(concepts_sorted[0:2])\n",
    "\n",
    "        shortest = concepts_sorted[0:3] #\n",
    "        for c in shortest:\n",
    "            if c in blacklist:\n",
    "                continue\n",
    "            lcs = lemmatize(nlp, c)\n",
    "            intersect = lcs.intersection(shortest)\n",
    "            if len(intersect)>0:\n",
    "                c = list(intersect)[0]\n",
    "                if c in concept2id:\n",
    "                    mentioned_concepts[span] = c\n",
    "                    break\n",
    "            else:\n",
    "                if c in concept2id:\n",
    "                    mentioned_concepts[span] = c\n",
    "                    break\n",
    "\n",
    "    \n",
    "    mentioned_concepts_with_indices = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end].text\n",
    "        if span in mentioned_concepts:\n",
    "            concept = mentioned_concepts[span]\n",
    "            concept_id = concept2id[concept]\n",
    "            mentioned_concepts_with_indices.append([start, end, span, concept, concept_id])\n",
    "\n",
    "    mentioned_concepts_with_indices = sorted(mentioned_concepts_with_indices, key=lambda x: (x[1],-x[0])) # sort based on end then start\n",
    "    \n",
    "    # mentioned_concepts_with_indice with filtered intersection\n",
    "    res = []\n",
    "    for mc in reversed(mentioned_concepts_with_indices):\n",
    "        if len(res) == 0:\n",
    "            res.append(mc)\n",
    "        elif mc[1] <= res[-1][0]: # no intersection between current concept, and last included concepts \n",
    "            res.append(mc)\n",
    "    \n",
    "    res.reverse()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def hard_ground(nlp, sent):\n",
    "    global cpnet_vocab\n",
    "    sent = sent.lower()\n",
    "    doc = nlp(sent)\n",
    "    res = []\n",
    "    for idx, t in enumerate(doc):\n",
    "        if t.lemma_ in cpnet_vocab and t.lemma_ in concept2id:\n",
    "            concept_id = concept2id[t.lemma_]\n",
    "            res.append([idx, idx + 1, str(t), str(t.lemma_), concept_id])\n",
    "    return res\n",
    "\n",
    "def match_mentioned_concepts(nlp, matcher, sent):\n",
    "    # print(\"Begin matching concepts.\")\n",
    "    all_concepts = ground_mentioned_concepts(nlp, matcher, sent)\n",
    "    if len(all_concepts)==0:\n",
    "        all_concepts = hard_ground(nlp, sent) # not very possible\n",
    "        print('hard ground', sent)\n",
    "\n",
    "    return all_concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, is, 'be', 1452], [2, 3, owned, 'own', 395], [3, 4, by, 'by', 2749]]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hard_ground(nlp, sent):\n",
    "    global cpnet_vocab\n",
    "    sent = sent.lower()\n",
    "    doc = nlp(sent)\n",
    "    res = []\n",
    "    for idx, t in enumerate(doc):\n",
    "        if t.lemma_ in cpnet_vocab and t.lemma_ in concept2id:\n",
    "            concept_id = concept2id[t.lemma_]\n",
    "            res.append([idx, idx + 1, t, str(t.lemma_), concept_id])\n",
    "    return res\n",
    "\n",
    "st = 'Telemundo is owned by ESPN.'\n",
    "hard_ground(nlp, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debonding Abaddon debonding abaddon remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\n",
      "[[1, 2, 'abaddon', 'abaddon', 79949], [3, 4, 'abaddon', 'abaddon', 79949], [4, 5, 'remained', 'remain', 3085], [6, 7, 'team', 'team', 1736], [8, 9, 'starting', 'start', 13762], [9, 10, 'quarterback', 'quarterback', 48514], [12, 13, 'rest', 'rest', 309], [15, 16, 'season', 'season', 6760], [17, 19, 'went on', 'go_on', 9784], [20, 21, 'lead', 'lead', 7235], [24, 25, 'their', 'mine', 19735], [25, 26, 'first', 'first', 3945], [26, 28, 'super bowl', 'super_bowl', 199537], [28, 29, 'appearance', 'appearance', 1263], [29, 30, 'since', 'since', 14293], [32, 33, 'losing', 'lose', 4705], [35, 36, 'baltimore', 'baltimore', 18538]]\n"
     ]
    }
   ],
   "source": [
    "s = \"Debonding Abaddon debonding abaddon remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\"\n",
    "print(nlp(s))\n",
    "concepts = match_mentioned_concepts(nlp, matcher, sent=s)\n",
    "print(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../../bert_base/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['De', '##bon', '##ding', 'A', '##bad', '##don', 'de', '##bon', '##ding', 'a', '##bad', '##don', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '1994', ',', 'losing', 'to', 'the', 'Baltimore', 'Ravens', '.'] 47\n",
      "[3177, 8868, 3408, 138, 8330, 3842, 1260, 8868, 3408, 170, 8330, 3842, 1915, 1103, 1264, 112, 188, 2547, 9119, 1111, 1103, 1832, 1104, 1103, 1265, 1105, 1355, 1113, 1106, 1730, 1103, 19531, 1106, 1147, 1148, 3198, 5308, 2468, 1290, 1898, 117, 3196, 1106, 1103, 5553, 21848, 119] 47\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(s)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(tokens, len(tokens))\n",
    "print(input_ids, len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '##bad', '##don', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '1994', ',', 'losing', 'to', 'the', 'Baltimore', 'Ravens', '.'] 38\n",
      "[0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35] 38\n",
      "['Abaddon', 'remained', 'the', 'team', \"'\", 's', 'starting', 'quarterback', 'for', 'the', 'rest', 'of', 'the', 'season', 'and', 'went', 'on', 'to', 'lead', 'the', '49ers', 'to', 'their', 'first', 'Super', 'Bowl', 'appearance', 'since', '1994', ',', 'losing', 'to', 'the', 'Baltimore', 'Ravens', '.'] 36\n",
      "[[0, 1, 'abaddon', 'abaddon', 79949], [1, 2, 'remained', 'remain', 3085], [3, 4, 'team', 'team', 1736], [5, 6, 'starting', 'start', 13762], [6, 7, 'quarterback', 'quarterback', 48514], [9, 10, 'rest', 'rest', 309], [12, 13, 'season', 'season', 6760], [14, 16, 'went on', 'go_on', 9784], [17, 18, 'lead', 'lead', 7235], [21, 22, 'their', 'mine', 19735], [22, 23, 'first', 'first', 3945], [23, 25, 'super bowl', 'super_bowl', 199537], [25, 26, 'appearance', 'appearance', 1263], [26, 27, 'since', 'since', 14293], [29, 30, 'losing', 'lose', 4705], [32, 33, 'baltimore', 'baltimore', 18538]] 16\n",
      "word = Abaddon, concept = [0, 1, 'abaddon', 'abaddon', 79949]\n",
      "word = remained, concept = [1, 2, 'remained', 'remain', 3085]\n",
      "word = the, concept = NONE\n",
      "word = team, concept = [3, 4, 'team', 'team', 1736]\n",
      "word = ', concept = NONE\n",
      "word = s, concept = NONE\n",
      "word = starting, concept = [5, 6, 'starting', 'start', 13762]\n",
      "word = quarterback, concept = [6, 7, 'quarterback', 'quarterback', 48514]\n",
      "word = for, concept = NONE\n",
      "word = the, concept = NONE\n",
      "word = rest, concept = [9, 10, 'rest', 'rest', 309]\n",
      "word = of, concept = NONE\n",
      "word = the, concept = NONE\n",
      "word = season, concept = [12, 13, 'season', 'season', 6760]\n",
      "word = and, concept = NONE\n",
      "word = went, concept = [14, 16, 'went on', 'go_on', 9784]\n",
      "word = on, concept = [14, 16, 'went on', 'go_on', 9784]\n",
      "word = to, concept = NONE\n",
      "word = lead, concept = [17, 18, 'lead', 'lead', 7235]\n",
      "word = the, concept = NONE\n",
      "word = 49ers, concept = NONE\n",
      "word = to, concept = NONE\n",
      "word = their, concept = [21, 22, 'their', 'mine', 19735]\n",
      "word = first, concept = [22, 23, 'first', 'first', 3945]\n",
      "word = Super, concept = [23, 25, 'super bowl', 'super_bowl', 199537]\n",
      "word = Bowl, concept = [23, 25, 'super bowl', 'super_bowl', 199537]\n",
      "word = appearance, concept = [25, 26, 'appearance', 'appearance', 1263]\n",
      "word = since, concept = [26, 27, 'since', 'since', 14293]\n",
      "word = 1994, concept = NONE\n",
      "word = ,, concept = NONE\n",
      "word = losing, concept = [29, 30, 'losing', 'lose', 4705]\n",
      "word = to, concept = NONE\n",
      "word = the, concept = NONE\n",
      "word = Baltimore, concept = [32, 33, 'baltimore', 'baltimore', 18538]\n",
      "word = Ravens, concept = NONE\n",
      "word = ., concept = NONE\n"
     ]
    }
   ],
   "source": [
    "def bert_concept_alignment(tok2id, words, concepts):\n",
    "    word_id2concept_id = [0] * len(words)\n",
    "    for i in range(len(words)):\n",
    "        word_id2concept_id[i] = i\n",
    "    \n",
    "    concept_id = 0\n",
    "    s_idx = 0\n",
    "    for word_id, word in enumerate(words):\n",
    "        if concept_id == len(concepts):\n",
    "            word_id2concept_id[word_id] = -1\n",
    "            continue\n",
    "        word_concept = concepts[concept_id][2]\n",
    "        word_id2concept_id[word_id] = concept_id\n",
    "        cur_span = ' '.join(words[s_idx:(word_id + 1)]).lower()\n",
    "        \n",
    "#         print('concept_idx={}, concept={}, word_idx={}, word={}, cur_span={}, head_i={}'.format(concept_id, word_concept, word_id, word, cur_span, word_id2concept_id[word_id]))\n",
    "        if cur_span.lower() == word_concept.lower():\n",
    "            concept_id += 1\n",
    "            s_idx = word_id + 1\n",
    "        elif not word_concept.lower().startswith((cur_span+' ')): #current word does not belong to any concept\n",
    "            s_idx = word_id + 1\n",
    "            word_id2concept_id[word_id] = -1\n",
    "\n",
    "    for word_id, word in enumerate(words):\n",
    "        concept_id = word_id2concept_id[word_id]\n",
    "        concept = concepts[concept_id] if concept_id != -1 else 'NONE'\n",
    "        print('word = {}, concept = {}'.format(word, concept))\n",
    "    pass\n",
    "\n",
    "def combine_berttoken_concept(tokens, concepts):\n",
    "    \n",
    "    tok2id = [0] * len(tokens)\n",
    "    cur_indice = -1\n",
    "    \n",
    "    words = []\n",
    "    cur_word = ''\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if not token.startswith('##'):\n",
    "            if cur_indice != -1:\n",
    "                words.append(cur_word)\n",
    "            cur_indice += 1\n",
    "            cur_word = token\n",
    "        else:\n",
    "            cur_word += token[2:]\n",
    "        tok2id[idx] = cur_indice\n",
    "    words.append(cur_word)\n",
    "    \n",
    "    print(tokens, len(tokens))\n",
    "    print(tok2id, len(tok2id))\n",
    "    print(words, len(words))\n",
    "    print(concepts, len(concepts))\n",
    "    \n",
    "    tok2id = bert_concept_alignment(tok2id, words, concepts)\n",
    "    \n",
    "combine_berttoken_concept(tokens, concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_need_append_token(span, concept):\n",
    "    span = span.lower()\n",
    "    word_concept = concept[2].lower() # get the ori word, not the concept\n",
    "    \n",
    "    if span == word_concept: # exact match, no need to append more token\n",
    "        return False\n",
    "    if not word_concept.startswith(span+' '): # current span is completly different with the concept. no need to append\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def match_span_concept(span, concept):\n",
    "    span = span.lower()\n",
    "    word_concept = concept[2].lower() # get the ori word, not the concept\n",
    "    \n",
    "    if span == word_concept:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def bert_concept_alignment(tokens, concepts):\n",
    "    n_token = len(tokens)\n",
    "    n_concept = len(concepts)\n",
    "    \n",
    "    token_id2concept_id = np.zeros((n_token), dtype=int)\n",
    "    tok2id = np.zeros((n_token), dtype=int)\n",
    "    for i in range(n_token):\n",
    "        token_id2concept_id[i] = i\n",
    "        tok2id[i] = i\n",
    "        \n",
    "    s_id = 0\n",
    "    e_id = 0\n",
    "    concept_id = 0\n",
    "    current_span = ''\n",
    "    while s_id != n_token:\n",
    "        # process sub-word level\n",
    "        next_token_id = s_id + 1\n",
    "        current_span = tokens[s_id]\n",
    "        while next_token_id < n_token and tokens[next_token_id].startswith('##'):\n",
    "            current_span += tokens[next_token_id][2:] # remove ##\n",
    "            next_token_id += 1\n",
    "        \n",
    "        # let's see if combining next token will form a concept\n",
    "        e_id = next_token_id\n",
    "        while concept_id < n_concept and do_need_append_token(current_span, concepts[concept_id]):\n",
    "            current_span += (' ' + tokens[e_id])\n",
    "            e_id += 1\n",
    "        \n",
    "        # if current span match with current concept\n",
    "        if concept_id < n_concept and match_span_concept(current_span, concepts[concept_id]):\n",
    "            token_id2concept_id[s_id:e_id] = concept_id\n",
    "            concept_id += 1\n",
    "        else:\n",
    "            token_id2concept_id[s_id:e_id] = -1\n",
    "            \n",
    "        s_id = e_id\n",
    "    \n",
    "    assert concept_id == n_concept\n",
    "    \n",
    "    cur_id = -1\n",
    "    for idx in range(n_token):\n",
    "        if token_id2concept_id[idx] == -1:\n",
    "            cur_id += 1\n",
    "        elif idx == 0 or token_id2concept_id[idx] != token_id2concept_id[idx - 1]:\n",
    "            cur_id += 1\n",
    "        \n",
    "        tok2id[idx] = cur_id\n",
    "    \n",
    "    # merge same token into span\n",
    "    final_tok2id = list(range(0, tok2id[-1] + 1))\n",
    "    final_token_id2concept_id = []\n",
    "    for idx in range(n_token):\n",
    "        if idx == 0 or tok2id[idx] == -1 or tok2id[idx] != tok2id[idx - 1]:\n",
    "            final_token_id2concept_id.append(token_id2concept_id[idx])\n",
    "    \n",
    "    assert len(final_tok2id) == len(final_token_id2concept_id)\n",
    "    \n",
    "    # create token pooling mask to pool subword level to span level. we use average pooling\n",
    "    token_pooling_mask = []\n",
    "    s_id = 0\n",
    "    e_id = 0\n",
    "    while s_id != n_token:\n",
    "        e_id = s_id + 1\n",
    "        \n",
    "        while e_id < n_token and tok2id[s_id] != -1 and tok2id[s_id] == tok2id[e_id]:\n",
    "            e_id += 1\n",
    "\n",
    "        mask = np.zeros((n_token), dtype=float)\n",
    "        n = e_id - s_id\n",
    "        mask[s_id:e_id] = (1/n)\n",
    "        token_pooling_mask.append(mask)\n",
    "        s_id = e_id\n",
    "    \n",
    "    assert len(token_pooling_mask) == len(final_tok2id)\n",
    "    return final_tok2id, final_token_id2concept_id, token_pooling_mask\n",
    "\n",
    "def combine_berttoken_concept(tokens, concepts):\n",
    "    tok2id = bert_concept_alignment(tokens, concepts)\n",
    "    \n",
    "combine_berttoken_concept(tokens, concepts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kagnet_test",
   "language": "python",
   "name": "kagnet_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
